# 这是一个爬取各网站网址的练习项目
## 安装依赖
1. python3.x
2. requests, bs4, win_unicode_console(用来解决命令行转码问题的模块)

## 设计思路
+ 利用百度的搜索功能，以及优秀的搜索算法，一般百度搜索出的第一个都是官网地址。但还是有一些网站甚至找不到它的官网地址...
+ 用BeautifulSoup解析页面，通过对页面的DOM元素的观察，找出网址URL所在的元素，提取出URL。
+ 将得到的结果写入到csv中

## 遇到的坑
1. 结果的url太长，百度会用省略号隐藏。这时候就要把百度设置的跳转url保存下来，访问这个链接，然后在得到的临时跳转页面中找到真正的url。
2. 访问过于频繁，百度会拒绝访问。需要设置线程锁，将同时运行的线程限制到5个以内，并且在每个线程中设置sleep。
3. 有一些url中会包含协议http或者https，要处理掉（也可以不处理）
4. 由于还涉及到了文件的I/O操作，如果在子线程完成前，主线程就进行了文件读写，就会出现结果不全的bug。因此每个线程在start之后，要调用其join方法阻塞主线程的执行。在所有子线程完成了之后，主线程才继续执行。
5. 文件读写中有\Xa0这种奇怪的字符，无法写入。
	+ 解决方法一：强行将得到的字符串强行encode成gbk，且在遇到无法转换的字符时ignore。问题是写入文件时会有奇怪的格式，比如b'...'
	+ 解决方法二：打开写入文件的时候进行utf-8转码。强烈不建议，最后打开文件看到的结果会是奇怪的人类都不懂的编码。
	+ 解决方法三：仔细观察后发现，这个无法转换的字符只有在每个字符串的末尾有，因此，每次存字符串的时候，忽略最后一个字符就好了！简单粗暴（采用）

## 更新
+ initial 18/4/15